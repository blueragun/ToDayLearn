# ch06장 실습



## 스파크 스트리밍

- Spark Streaming은 라이브 데이터 스트림의 확장이 가능하고 처리량이 높다.

  -  특정시간 간격 내에 유입된 데이터 블록을 RDD로 구성함 // 미니배치

- 내결함성 스트림 처리를 가능하게 하는 핵심 Spark API의 확장

  - 내결함성 : 시스템의 일부 구성요소에 장애가 있어도 시스템이 계속 작동할 수 있는 기능

- mapKfka, Kinesis 또는 TCP 소켓과 같은 다양한 소스에서 데이터 수집 가능

- reduce, join와 같은 고급 기능으로 표현된 복잡한 알고리즘 사용 가능

- 데이터를 파일 시스템, 데이터베이스 및 라이브 대시보드로 푸시할 수 있다

  

## 예제

- 증권사 대시보드 애플리케이션 구축 중 초당거래 주문건수, 누적거래액 1~5위, 지난 1시간 동안 거래량이 가장 많았던 유가 증권 1~ 5위 집계

  

### 스트리밍 컨텍스트 생성

```spark
sprak-shell --master local[4]     # 로컬클러스터 사용할 떄 사용

import org.apache.spark._
import org.apache.spark.streaming._

val ssc = new StreamingContext(sc, Seconds(5 ))           # Seconds(5) : 미니배치 RDD 생성시간 간격 지정

cd first-edition/ch06          # 경로 바꾸기
 tar xvfs orders.tar.gz        # 알집 압축풀기

val filestream = ssc.textFileStream("/home/spark/ch06input")      # 스트리밍 애플리케이션의 입력폴더 설정
```

####  데이터 파싱

```spark
import java.sql.Timestamp
case class Order(time: java.sql.Timestamp, orderId:Long, clientId:Long, symbol:String, amount:Int, price:Double, buy:Boolean)

import java.text.SimpleDateFormat

val orders = filestream.flatMap(line => {
    val dateFormat = new SimpleDateFormat("yyyy-MM-dd hh:mm:ss")
    val s = line.split(",")
    try {
        assert(s(6) == "B" || s(6) == "S")
        List(Order(new Timestamp(dateFormat.parse(s(0)).getTime()), s(1).toLong, s(2).toLong, s(3), s(4).toInt, s(5).toDouble, s(6) == "B"))
    }
    catch {
        case e : Throwable => println("Wrong line format ("+e+"): "+line)
        List()
    }
})
```

#### 거래 주문 건수 집계

```spark
val numPerType = orders.map(o => (o.buy, 1L)).reduceByKey((c1, c2) => c1+c2)
```

#### 결과를 파일로 저장

```spark
numPerType.repartition(1).saveAsTextFiles("/home/spark/ch06output/output", "txt")
```

#### 스트리밍 계산 작업 시작

```spark
ssc.start()
```

#### 스파크 스트리밍으로 데이터 전송

```spark
chmod +x first-edition/ch06/splitAndSend.sh   # splitAndSend.sh를 사용해 데이터 밀어넣기

mkdir /home/spark/ch06input

cd first-edition/ch06

./splitAndSend.sh /home/spark/ch06input local
```

#### 스타트 후 위 경로에 파일 무한 생성

```spark
ssc.stop(false)
```

#### 생성 중지

```spark
val allCounts = sc.textFile("/home/spark/ch06output/output*.txt")
```

- 집계결과는 part-00000 과 _SUCCESS 파일로 나타남
  - park-00000 : (false,9969) (true,10031)  

